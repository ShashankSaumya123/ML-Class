{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as Random\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r\"E:\\My work\\Sem 7\\ML\\ML-Class\\End Sem\\End sem Data.xlsx\")\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(data)\n",
    "Y = data[:,-1]\n",
    "X = np.delete(data,-1,1)\n",
    "X = X.astype(np.float64)\n",
    "Y = Y.astype(np.float64)\n",
    "#Separating into train/test (80-20)\n",
    "sep = 0.8*len(X)-1\n",
    "sep = int(sep)\n",
    "trainx, testx = X[:sep,:], X[sep:,:]\n",
    "trainy, testy = Y[:sep], Y[sep:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GD from scratch\n",
    "def GD_Batch(x,y, epochs = 10**4, α = 0.001):\n",
    "    w0 = np.ones((len(x),1))\n",
    "    x = np.hstack((w0,x))\n",
    "    wc = np.array([0]*len(x[0]))\n",
    "    n = len(x)\n",
    "    for epoch in range(epochs):\n",
    "        yp = np.dot(x,wc)\n",
    "\n",
    "        #Reshaping y to allow for multiplication\n",
    "        yp = yp.reshape(len(yp),1)\n",
    "        y = y.reshape(len(y),1)\n",
    "        ws = (x*(y-yp)).sum(axis=0)\n",
    "        wd = -(2/n)*ws\n",
    "        wc = wc-(α*wd)\n",
    "        #print(\"m {}, b {}, iteration {}\".format(mc,bc,i))\n",
    "    return(wc)\n",
    "\n",
    "# GD with L1 regularisation\n",
    "def GD_BatchL1(x,y, epochs = 10**4, α = 0.001, λ = 0.1):\n",
    "    w0 = np.ones((len(x),1))\n",
    "    x = np.hstack((w0,x))\n",
    "    wc = np.array([0.5]*len(x[0]))\n",
    "    n = len(x)\n",
    "    for epoch in range(epochs):\n",
    "        yp = np.dot(x,wc)\n",
    "\n",
    "        #Reshaping y to allow for multiplication\n",
    "        yp = yp.reshape(len(yp),1)\n",
    "        y = y.reshape(len(y),1)\n",
    "        ws = (x*(y-yp)).sum(axis=0)\n",
    "        wd = -(2/n)*ws\n",
    "        ld = λ*np.sign(wc)\n",
    "        wc = wc-α*(wd+ld)\n",
    "        #print(\"m {}, b {}, iteration {}\".format(mc,bc,i))\n",
    "    return(wc)\n",
    "\n",
    "# GD with L2 regularisation\n",
    "def GD_BatchL2(x,y, epochs = 10**4, α = 0.001, λ = 0.1):\n",
    "    w0 = np.ones((len(x),1))\n",
    "    x = np.hstack((w0,x))\n",
    "    wc = np.array([0]*len(x[0]))\n",
    "    n = len(x)\n",
    "    for epoch in range(epochs):\n",
    "        yp = np.dot(x,wc)\n",
    "\n",
    "        #Reshaping y to allow for multiplication\n",
    "        yp = yp.reshape(len(yp),1)\n",
    "        y = y.reshape(len(y),1)\n",
    "        ws = (x*(y-yp)).sum(axis=0)\n",
    "        wd = -(2/n)*ws\n",
    "        ld = 2*λ*wc\n",
    "        wc = wc-α*(wd+ld)\n",
    "        #print(\"m {}, b {}, iteration {}\".format(mc,bc,i))\n",
    "    return(wc)\n",
    "\n",
    "#SGD only works for 5 feature vector data\n",
    "def SGD(x,y, iter = 10**4, α = 0.001):\n",
    "    y = y.tolist()\n",
    "    wc1 = wc2 = wc3 = wc4 = wc5 = bc = 0\n",
    "    n = len(x)\n",
    "    for i in range(iter):\n",
    "        ya = np.random.choice(y)\n",
    "        xa1 = x[:,0]\n",
    "        xa1 = xa1[y.index(ya)]\n",
    "        xa2 = x[:,1]\n",
    "        xa2 = xa2[y.index(ya)]\n",
    "        xa3 = x[:,2]\n",
    "        xa3 = xa3[y.index(ya)]\n",
    "        xa4 = x[:,3]\n",
    "        xa4 = xa4[y.index(ya)]\n",
    "        xa5 = x[:,4]\n",
    "        xa5 = xa5[y.index(ya)]\n",
    "\n",
    "        yp = (wc1*xa1)+(wc2*xa2)+(wc3*xa3)+(wc4*xa4)+(wc5*xa5)+bc\n",
    "\n",
    "        wd1 = -2*xa1*(ya-yp)\n",
    "        wd2 = -2*xa2*(ya-yp)\n",
    "        wd3 = -2*xa3*(ya-yp)\n",
    "        wd4 = -2*xa4*(ya-yp)\n",
    "        wd5 = -2*xa5*(ya-yp)\n",
    "        bd = -2*(ya-yp)\n",
    "        wc1 = wc1-(α*wd1)\n",
    "        wc2 = wc2-(α*wd2)\n",
    "        wc3 = wc3-(α*wd3)\n",
    "        wc4 = wc4-(α*wd4)\n",
    "        wc5 = wc5-(α*wd5)\n",
    "        bc = bc-(α*bd)\n",
    "    wc = [bc,wc1,wc2,wc3,wc4,wc5]\n",
    "    return(wc)\n",
    "    \n",
    "    \n",
    "    \n",
    "# Mini Batch GD (mini batch size 20) only works for 5 feature vector data\n",
    "def minBatchGD(x,y,iter = 10**4,α = 0.001,bs = 3):\n",
    "    y = y.tolist()\n",
    "    wc1 = wc2 = wc3 = wc4 = wc5 = bc = 0\n",
    "    n = len(x)\n",
    "    for i in range(iter):\n",
    "        ya = Random.sample(y,k = bs)\n",
    "        xa1 = np.zeros(bs)\n",
    "        xa2 = np.zeros(bs)\n",
    "        xa3 = np.zeros(bs)\n",
    "        xa4 = np.zeros(bs)\n",
    "        xa5 = np.zeros(bs)\n",
    "\n",
    "        xas1 = x[:,0]\n",
    "        xas2 = x[:,1]\n",
    "        xas3 = x[:,2]\n",
    "        xas4 = x[:,3]\n",
    "        xas5 = x[:,4]\n",
    "        for i in range(bs):\n",
    "            xa1[i] = xas1[y.index(ya[i])]\n",
    "            xa2[i] = xas2[y.index(ya[i])]\n",
    "            xa3[i] = xas3[y.index(ya[i])]\n",
    "            xa4[i] = xas4[y.index(ya[i])]\n",
    "            xa5[i] = xas5[y.index(ya[i])]\n",
    "            \n",
    "        yp = (wc1*xa1)+(wc2*xa2)+(wc3*xa3)+(wc4*xa4)+(wc5*xa5)+bc\n",
    "\n",
    "        wd1 = -(2/bs)*sum(xa1*(ya-yp))\n",
    "        wd2 = -(2/bs)*sum(xa2*(ya-yp))\n",
    "        wd3 = -(2/bs)*sum(xa3*(ya-yp))\n",
    "        wd4 = -(2/bs)*sum(xa4*(ya-yp))\n",
    "        wd5 = -(2/bs)*sum(xa5*(ya-yp))\n",
    "        bd = -(2/bs)*sum(ya-yp)\n",
    "\n",
    "        wc1 = wc1-(α*wd1)\n",
    "        wc2 = wc2-(α*wd2)\n",
    "        wc3 = wc3-(α*wd3)\n",
    "        wc4 = wc4-(α*wd4)\n",
    "        wc5 = wc5-(α*wd5)\n",
    "        bc = bc-(α*bd)\n",
    "    \n",
    "    wc = [bc,wc1,wc2,wc3,wc4,wc5]\n",
    "    return(wc)\n",
    "\n",
    "# Predicts the values based on weights\n",
    "def predict(x,w):\n",
    "    x0 = np.ones((len(x),1))\n",
    "    x = np.hstack((x0,x))\n",
    "    pred = np.dot(x,w)\n",
    "    return(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularisation is adding λ*L\n",
    "# L1 means L is the L1 norm of w\n",
    "# L2 means L is the L2 norm of w\n",
    "# So, the loss function changes.\n",
    "# The derivative for L1 becomes (sign of w)*λ\n",
    "# The derivative for L2 becomes 2*λ*w\n",
    "\n",
    "# When there are very high powers in the fiting function of the model, it has low bias but high variance (overfiting).\n",
    "# And when it has only lower powers (say linear regression), it has low vaiance but high bias (underfit).\n",
    "# Regularisation can help decrease variance by making the higher powers have small weights. Thus keeping it low bias and low variance.\n",
    "# L2 reduces the higher power weights faster than L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying linear regression using sklearn (for comparison)\n",
    "model = LinearRegression()\n",
    "res = model.fit(trainx, trainy)\n",
    "predy = res.predict(testx)\n",
    "\n",
    "# Linear Regression using my code (epochs = 10**4, α = 0.001, λ = 0.1)\n",
    "w1 = GD_Batch(trainx,trainy)\n",
    "w2 = GD_BatchL1(trainx,trainy)\n",
    "w3 = GD_BatchL2(trainx,trainy)\n",
    "\n",
    "predy1 = predict(testx,w1)\n",
    "predy2 = predict(testx,w2)\n",
    "predy3 = predict(testx,w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " 0.004362905400451141 [-0.59726041 -0.3591826  -0.49555989  0.23091207 -0.29541439]\n",
      "[ 0.00431022 -0.59629056 -0.35187967 -0.49269516  0.23013016 -0.30107666]\n",
      "[-7.29093744e-05 -5.00738371e-01 -1.93250527e-01 -3.68320927e-01\n",
      "  1.57074185e-01 -3.19465309e-01]\n",
      "[ 0.00090742 -0.51767305 -0.257408   -0.40577275  0.1942415  -0.3026978 ]\n",
      "Mean squared error (sklearn): 0.47\n",
      "Mean squared error (Normal): 0.47\n",
      "Mean squared error (L1): 0.47\n",
      "Mean squared error (L2): 0.47\n",
      "\n",
      "R2 score (sklearn): 0.43\n",
      "R2 score (normal): 0.43\n",
      "R2 score (L1): 0.43\n",
      "R2 score (L2): 0.44\n"
     ]
    }
   ],
   "source": [
    "# The coefficients\n",
    "print(\"Coefficients: \\n\", model.intercept_, model.coef_)\n",
    "print(w1)\n",
    "print(w2)\n",
    "print(w3)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error (sklearn): %.2f\" % mean_squared_error(testy, predy))\n",
    "print(\"Mean squared error (Normal): %.2f\" % mean_squared_error(testy, predy1))\n",
    "print(\"Mean squared error (L1): %.2f\" % mean_squared_error(testy, predy2))\n",
    "print(\"Mean squared error (L2): %.2f\" % mean_squared_error(testy, predy3))\n",
    "print()\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"R2 score (sklearn): %.2f\" % r2_score(testy, predy))\n",
    "print(\"R2 score (normal): %.2f\" % r2_score(testy, predy1))\n",
    "print(\"R2 score (L1): %.2f\" % r2_score(testy, predy2))\n",
    "print(\"R2 score (L2): %.2f\" % r2_score(testy, predy3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varying the hyperparameters\n",
    "w4 = GD_BatchL1(trainx,trainy, λ = 1)\n",
    "w5 = GD_BatchL1(trainx,trainy, λ = 0.98)\n",
    "w6 = GD_BatchL1(trainx,trainy, λ = 0.5)\n",
    "\n",
    "w7 = GD_BatchL2(trainx,trainy, λ = 1)\n",
    "w8 = GD_BatchL2(trainx,trainy, λ = 0.98)\n",
    "w9 = GD_BatchL2(trainx,trainy, λ = 0.5)\n",
    "\n",
    "predy4 = predict(testx,w4)\n",
    "predy5 = predict(testx,w5)\n",
    "predy6 = predict(testx,w6)\n",
    "\n",
    "predy7 = predict(testx,w7)\n",
    "predy8 = predict(testx,w8)\n",
    "predy9 = predict(testx,w9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error (L1, 0.1): 0.47\n",
      "Mean squared error (L1, 1): 0.84\n",
      "Mean squared error (L1, 0.98): 0.84\n",
      "Mean squared error (L1, 0.5): 0.69\n",
      "\n",
      "Mean squared error (L2, 0.1): 0.47\n",
      "Mean squared error (L2, 1): 0.57\n",
      "Mean squared error (L2, 0.98): 0.57\n",
      "Mean squared error (L2, 0.5): 0.51\n",
      "\n",
      "R2 score (L1, 0.1): 0.43\n",
      "R2 score (L1, 1): -0.01\n",
      "R2 score (L1, 0.98): -0.01\n",
      "R2 score (L1, 0.5): 0.17\n",
      "\n",
      "R2 score (L2, 0.1): 0.44\n",
      "R2 score (L2, 1): 0.31\n",
      "R2 score (L2, 0.98): 0.31\n",
      "R2 score (L2, 0.5): 0.38\n"
     ]
    }
   ],
   "source": [
    "# The mean squared error\n",
    "print(\"Mean squared error (L1, 0.1): %.2f\" % mean_squared_error(testy, predy2))\n",
    "print(\"Mean squared error (L1, 1): %.2f\" % mean_squared_error(testy, predy4))\n",
    "print(\"Mean squared error (L1, 0.98): %.2f\" % mean_squared_error(testy, predy5))\n",
    "print(\"Mean squared error (L1, 0.5): %.2f\" % mean_squared_error(testy, predy6))\n",
    "print()\n",
    "print(\"Mean squared error (L2, 0.1): %.2f\" % mean_squared_error(testy, predy3))\n",
    "print(\"Mean squared error (L2, 1): %.2f\" % mean_squared_error(testy, predy7))\n",
    "print(\"Mean squared error (L2, 0.98): %.2f\" % mean_squared_error(testy, predy8))\n",
    "print(\"Mean squared error (L2, 0.5): %.2f\" % mean_squared_error(testy, predy9))\n",
    "print()\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print(\"R2 score (L1, 0.1): %.2f\" % r2_score(testy, predy2))\n",
    "print(\"R2 score (L1, 1): %.2f\" % r2_score(testy, predy4))\n",
    "print(\"R2 score (L1, 0.98): %.2f\" % r2_score(testy, predy5))\n",
    "print(\"R2 score (L1, 0.5): %.2f\" % r2_score(testy, predy6))\n",
    "print()\n",
    "print(\"R2 score (L2, 0.1): %.2f\" % r2_score(testy, predy3))\n",
    "print(\"R2 score (L2, 1): %.2f\" % r2_score(testy, predy7))\n",
    "print(\"R2 score (L2, 0.98): %.2f\" % r2_score(testy, predy8))\n",
    "print(\"R2 score (L2, 0.5): %.2f\" % r2_score(testy, predy9))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c87a2612ab2f68c8a6eaedcbb1976cb87dbe57fd6798182a189e946bc46e8937"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('ML-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
